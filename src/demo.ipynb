{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Demo for KI Seminar\n",
    "\n",
    "## Content to create\n",
    "\n",
    "- Podcast script\n",
    "- Presentation\n",
    "- Code Demo?\n",
    "\n",
    "## Sequence - Podcast Script\n",
    "\n",
    "- Ask for the topic\n",
    "- Figure out the requirements\n",
    "  - How long does it need to be (time wise)\n",
    "    - (google wpm talking speed)\n",
    "    - calculate the words to time with wpm talking speed\n",
    "  - Topics (How do they work?, ChatGPT vs open models, ...)\n",
    "- Gather information on the topic and requirements (google search or wikipedia)\n",
    "- output information\n",
    "  - maybe to file even?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting ideas / modifications\n",
    "\n",
    "- could generate an interesting topic inside the domain of large language models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "import wikipedia\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup chatgpt\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.9)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the topic for which we want to create the podcast for\n",
    "topic = 'Large Language Models'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering information from the lecture notes\n",
    "\n",
    "We can use ChatGPT to interpret the lecture slides which lists out the requirements for the podcast.\n",
    "By loading this document we can figure out:\n",
    "\n",
    "- the duration which the podcast should have (20 min)\n",
    "- How many words the use in the script for the podcast\n",
    "- Which topics should be covered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document loaders\n",
    "loader = TextLoader(file_path='../documents/VL1-1-processed-eng.txt', encoding='utf-8')\n",
    "document_content = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n",
    "split_content = text_splitter.split_documents(document_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x7f81e3a37430>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embeddings\n",
    "if os.path.isfile('../objects/embeddings.pkl'):\n",
    "    with open('../objects/embeddings.pkl', 'rb') as embeddings_file:\n",
    "        embeddings = pickle.load(embeddings_file)\n",
    "else:\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)  # type: ignore\n",
    "embeddings_search = Chroma.from_documents(split_content, embeddings)\n",
    "embeddings_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt template to get usable results\n",
    "prompt_template_text_document = \"\"\"\n",
    "Instruction:\n",
    "- Use the following pieces of context to answer the question at the end.\n",
    "- If you don't know the answer output: NULL\n",
    "- Just answer the question without providing any additional information\n",
    "\n",
    "Context:\n",
    "    {context}\n",
    "\n",
    "Question:\n",
    "    {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_documents = PromptTemplate(template=prompt_template_text_document, input_variables=['context', 'question'])\n",
    "chain_type_kwargs = {'prompt': prompt_template_documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever\n",
    "# usage with prompt templates see: https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
    "# to see the source documents set: return_source_documents=True\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=embeddings_search.as_retriever() chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Approx. 20 minutes'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the duration of the podcast in minutes\n",
    "query_duration = 'Which time duration should the podcast have?'\n",
    "res_duration = qa.run(query_duration)\n",
    "res_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_text_num_words = \"\"\"\n",
    "Instructions:\n",
    "  - if you are not sure make an estimate\n",
    "  - output just a number\n",
    "  - don't use any text in the answer like for example \"aproximately\" or \"words\"\n",
    "\n",
    "Context:\n",
    "  {context}\n",
    "\n",
    "Question:\n",
    "  How many words should I use in my podcast script to be able to talk the entire duration?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template_num_words = PromptTemplate(template=prompt_template_text_num_words, input_variables=['context'])\n",
    "num_words_chain = LLMChain(llm=llm, prompt=prompt_template_num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1200'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ask chatgpt how many words to use for the podcast\n",
    "res_words = num_words_chain.run(context=res_duration)\n",
    "res_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Use the output prompt to match the number of words and convert it to a int value\n",
    "\n",
    "    :param text: the prompt the tells us the number of words to use\n",
    "    :returns: the number specified in the text or 1000 if no number could be found\n",
    "    \"\"\"\n",
    "    max_tokens = 2048\n",
    "    regex = r\"\\b(\\d+[,.]\\d+|\\d+)\\b\"\n",
    "    results: list[str] = re.findall(regex, text, re.MULTILINE)\n",
    "    if not results:\n",
    "        return max_tokens\n",
    "\n",
    "    no_comma = re.sub(r'[,]', '', results[-1])\n",
    "    float_result = float(no_comma)\n",
    "    suggestion = round(float_result)\n",
    "    return max_tokens if suggestion > max_tokens else suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the output from chatgpt to an integer\n",
    "num_words_to_use = get_num_words(res_words)\n",
    "num_words_to_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How do LLMs work? ChatGPT vs open models? How are LLMs changing our everyday lives?'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query for the topics which should be used in the podcast\n",
    "query_topics = f'Which topics should be covered in the podcast about {topic}?'\n",
    "res_topics = qa.run(query_topics)\n",
    "res_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do LLMs work',\n",
       " 'ChatGPT vs open models',\n",
       " 'How are LLMs changing our everyday lives']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format the topics in a way to be interpretable\n",
    "formatted_topics = re.split(r'[,|\\n|?]', res_topics)\n",
    "formatted_topics = [topic.strip() for topic in formatted_topics if not re.search(r'^$', topic)]\n",
    "formatted_topics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Doing research about the topics\n",
    "\n",
    "Now that we found out which requirements and topics we should use we can now do research regarding these.\n",
    "For this we can use a variety of tools (also langchain integrations), but here we chose to use the wikipedia api.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_wikipedia(topic: str):\n",
    "    \"\"\"\n",
    "    Ask wikipedia for the summary of the topic.\n",
    "    Note: results my be inaccurate!\n",
    "\n",
    "    :param topic: the topic to ask wikipedia for\n",
    "    :returns: the summary of the topic or an empty string if nothing was found\n",
    "    \"\"\"\n",
    "    time.sleep(0.3)\n",
    "    search = wikipedia.search(topic)\n",
    "    if not search:\n",
    "        return ''\n",
    "\n",
    "    try:\n",
    "        return wikipedia.summary(search[0], sentences=5)\n",
    "    except wikipedia.PageError:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.Though the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.Though trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language.\\n\\nRaymond Walter Kelly (born September 4, 1941) is the longest-serving Commissioner in the history of the New York City Police Department (NYPD) and the first person to hold the post for two non-consecutive tenures. According to its website, Kelly, a lifelong New Yorker, had spent 45 years in the NYPD, serving in 25 different commands and as Police Commissioner from 1992 to 1994 and again from 2002 until 2013. Kelly was the first man to rise from Police Cadet to Police Commissioner, holding all of the department's ranks, except for Three-Star Bureau Chief, Chief of Department and Deputy Commissioner, having been promoted directly from Two-Star Chief to First Deputy Commissioner in 1990.\\nAfter his handling of the World Trade Center bombing in 1993, he was mentioned for the first time as a possible candidate for FBI Director. After Kelly turned down the position, Louis Freeh was appointed.Kelly was a Marine Corps Reserve colonel, director of police under the United Nations Mission in Haiti, and an Interpol vice president.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_research = '\\n'.join([ask_wikipedia(topic) for topic in formatted_topics])\n",
    "wiki_research\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate podcast script\n",
    "\n",
    "Now that we have done our research about the topic we can generate our podcast script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_text_script = \"\"\"\n",
    "Sub Topics:\n",
    "  {sub_topics}\n",
    "\n",
    "Context:\n",
    "  \\\"{context}\\\"\n",
    "\n",
    "Podcast Participants:\n",
    "  - Host\n",
    "  - Expert\n",
    "\n",
    "Previous Section of the Podcast Script:\n",
    "  \\\"{previous_section}\\\"\n",
    "\n",
    "Task:\n",
    "  - Your task is to write a podcast script about \\\"{topic}\\\".\n",
    "  - The Sub Topics refine the main topic and need to be addressed!\n",
    "  - Use your own knowledge and the one provided in Context if you think it fit the topic.\n",
    "  - Continue from the previous section and output the new content.\n",
    "  - If you think you are done output [END]\n",
    "\n",
    "\"\"\"\n",
    "prompt_template_podcast = PromptTemplate(template=prompt_template_text_script, input_variables=['sub_topics', 'context', 'topic', 'previous_section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_repeated_chain(chain: LLMChain, max_iterations: int = 8, stop_word: str = '[END]', **chain_kwargs) -> str:\n",
    "    \"\"\"\n",
    "    run the podcast chain until the podcast is the stop word is found or the max_iterations is reached\n",
    "\n",
    "    :param chain: the chain to run\n",
    "    :param max_iterations: the maximum number of iterations to run the chain\n",
    "    :param stop_word: the word to stop the chain\n",
    "    :param chain_kwargs: the kwargs to pass to the chain\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    found_end = False\n",
    "    script = ''\n",
    "    while not found_end:\n",
    "        if i >= 6:\n",
    "            print(f'[WARNING] could not finish task in {max_iterations} iterations')\n",
    "            break\n",
    "        output = chain.run(**chain_kwargs)\n",
    "        script += output\n",
    "        if 'previous_section' in chain_kwargs:\n",
    "            chain_kwargs['previous_section'] = output\n",
    "        found_end = stop_word in output\n",
    "        i += 1\n",
    "        print(f'iteration {i} of {max_iterations} finished')\n",
    "\n",
    "    return script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  - How do LLMs work\\n  - ChatGPT vs open models\\n  - How are LLMs changing our everyday lives'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_topics = '\\n'.join(f'  - {sub_topic}' for sub_topic in formatted_topics)\n",
    "sub_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 8 finished\n",
      "iteration 2 of 8 finished\n",
      "iteration 3 of 8 finished\n"
     ]
    }
   ],
   "source": [
    "podcast_chain = LLMChain(llm=llm, prompt=prompt_template_podcast)\n",
    "\n",
    "podcast_script = run_repeated_chain(podcast_chain, sub_topics=sub_topics, context=wiki_research, topic=topic, previous_section='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it to a file for later use\n",
    "with open('../out/podcast_script_wiki.txt', 'a') as script_file:\n",
    "    script_file.write(podcast_script)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tools and agents\n",
    "\n",
    "What we could do using other frameworks and manual methods like before (wikipedia), we can now use integrated tools in combination with agents to automate our process further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(['serpapi'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = [topic, *formatted_topics]\n",
    "web_research = []\n",
    "\n",
    "for topic in all_topics:\n",
    "    agent_res = agent.run(f\"Task: Do a thorough web research about {topic}. Provide at least 3 sentences of information.\")\n",
    "    web_research.append(agent_res)\n",
    "\n",
    "web_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) are recent advances in deep learning models to work on human languages. They are capable of understanding and generating text in a human-like fashion and can learn from context. Their key advantage is their ability to understand the meaning and intent behind words and phrases, allowing them to generate more accurate and appropriate responses. A number of real-world use cases have been demonstrated for LLMs.\\n\\nLLMs are a combination of machine learning and human input that use neural networks to process text data. They automate various tasks like document summarization, email drafting and content generation to benefit businesses by saving time and resources. LLMs can also integrate with organizational APIs for easy adoption.\\n\\nOpen models provide more flexibility and control than pre-trained models such as ChatGPT, making them better suited for scenarios where users need more control over the results. Pre-trained models like ChatGPT, however, are more accurate and are ideal when accuracy is a priority.\\n\\nLLMs are increasingly being used for content creation, customer service, and to provide career benefits such as furthering knowledge of the law, increasing job opportunities, gaining more respect in the profession, and more.'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_web_research = '\\n\\n'.join(web_research)\n",
    "formatted_web_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 8 finished\n",
      "iteration 2 of 8 finished\n",
      "iteration 3 of 8 finished\n",
      "iteration 4 of 8 finished\n",
      "iteration 5 of 8 finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nHost: Welcome back everyone, we are here today with [EXPERT] to talk about how large language models, or LLMs, are changing our everyday lives. First, can you give us a brief overview of LLMs and how they work?\\n\\nExpert: Sure. LLMs are a combination of machine learning and human input that use neural networks to process text data. They automate various tasks such as document summarization, email drafting, and content generation for businesses, which can save time and resources. They can also integrate with organizational APIs for easy adoption. LLMs have an advantage over pre-trained models such as ChatGPT because they provide more flexibility and control over the results. However, pre-trained models like ChatGPT are more accurate and are ideal when accuracy is a priority.\\n\\nHost: That's really interesting. What kind of everyday applications are there for LLMs?\\n\\nExpert: LLMs are being used in a variety of ways in our daily lives, particularly for content creation, customer service, and for career benefits. They are being used to generate content quickly, both through text and voice. In terms of customer service, LLMs can understand natural language questions and provide appropriate responses. They can also surface relevant information to\\nHost: We've heard how LLMs are useful for automated tasks like document summarization and content generation, but how else are they being used to improve our lives?\\n\\nExpert: LLMs are being used in many different ways. For example, they are being used to generate content quickly, both through text and voice, which can be a huge time-saver. LLMs are also being used in customer service as they can understand natural language questions and provide appropriate responses. This is particularly beneficial for businesses and organizations who need to provide quick and accurate customer service.\\n\\nLLMs are also providing career benefits. For instance, many lawyers are now taking courses that focus on teaching LLM use and deployment, which can increase their knowledge of the law and possibly open up job opportunities. Furthermore, having experience with using LLMs can increase a lawyer's respect in the profession and give them an advantage over others who have yet to gain the same level of experience.\\n\\nOverall, LLMs are providing us with a wealth of opportunities that were not available before. By improving communication, content creation, and customer service, LLMs are making life a lot easier for businesses and individuals alike.\\n\\nHost: That's incredible. [EXPERT], thank you so much\\nExpert: One of the most impactful uses of LLMs is in communication. LLMs are able to understand and generate written and spoken language in a more natural and human-like way. This can be beneficial for conversations between customers and businesses, as the responses generated by LLMs are more accurate and appropriate.\\n\\nLLMs are also becoming increasingly useful for content creation. They can be used to generate content quickly, such as articles, blog posts, and social media posts. By automating these tasks, businesses are able to save time and resources, and can focus more on other tasks and strategies.\\n\\nIn terms of consumer benefits, LLMs can provide customers with more personalized experiences. By understanding context and intent, LLMs can provide more accurate answers to customer inquiries and can provide suggestions based on past behavior and preferences.\\n\\nLastly, LLMs can help with career benefits. By gaining experience and knowledge on how to use and deploy LLMs, individuals, such as lawyers, can increase their knowledge of the law and gain a competitive advantage with regards to job opportunities. Furthermore, having experience with LLMs can give those individuals more respect in the profession.\\n\\nOverall, LLMs are having an increasingly large impact on our everyday lives. By improving communication,\\nHost: That's really interesting. So, let's dive deeper into how LLMs are changing our everyday lives. First of all, how do LLMs actually work?\\n\\nExpert: LLMs are a combination of machine learning and human input that use neural networks to process text data. For example, if you train an LLM on a dataset of legal documents, it can learn the relevant vocabulary, grammar, and syntax of the law. By understanding context and intent, LLMs can then be used to generate legal documents quickly and accurately.\\n\\nHost: That's incredible! So, can you tell us about the differences between pre-trained models like ChatGPT, and open models?\\n\\nExpert: Sure. Open models provide more flexibility and control than pre-trained models such as ChatGPT. Open models allow the user to customize the model to their own use case. However, pre-trained models like ChatGPT are more accurate and are ideal when accuracy is a priority.\\n\\nHost: Interesting. So, what kind of impact are LLMs having in terms of everyday life?\\n\\nExpert: LLMs are being used in a variety of fields. In terms of communication, LLMs can help automate customer service and provide\\nExpert: LLMs are having a huge impact on our day to day lives. For example, many businesses are using LLMs to automate tasks like natural language processing, document summarization, and even content generation. This saves precious time and resources for the companies who use them. LLMs can also integrate with organizational APIs for easy adoption.\\n\\nIn terms of career benefits, LLMs can allow legal professionals to gain a better understanding of the law and its many nuances, as well as providing a competitive edge over other professionals. They can also open up more job opportunities and provide increased respect in the profession.\\n\\nIn terms of communication, LLMs can help automate customer service, reduce response times, and provide more accurate answers, resulting in improved customer satisfaction.\\n\\nOverall, LLMs are becoming an invaluable tool that can help to improve productivity, accuracy, and efficiency in a range of industries. They are here to stay and are likely to have a huge impact on our lives in the coming years.\\n\\n[END]\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_chain = LLMChain(llm=llm, prompt=prompt_template_podcast)\n",
    "\n",
    "podcast_script_web = run_repeated_chain(podcast_chain, sub_topics=sub_topics, context=formatted_web_research, topic=topic, previous_section='')\n",
    "podcast_script_web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../out/podcast_script_web.txt', 'w') as script_file:\n",
    "    script_file.write(podcast_script_web)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Until now we have done everything in a manual way, but [LangChain](https://python.langchain.com/en/latest/index.html) provides us with a way\n",
    "to intermingle everything and create a truly autonomous agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: chaining\n",
    "sequential_chain = SimpleSequentialChain(chains=)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-ki-seminar-p9aLL8gI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
