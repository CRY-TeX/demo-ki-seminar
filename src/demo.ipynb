{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo _Seminar Aktuelle Themen der KI_\n",
    "\n",
    "This is a demo for the course _Seminar Aktuelle Themen der KI_, KI-B-6, TH Deggendorf for the topic \"T1: Large Language Models\".\n",
    "We are going to leverage ChatGPT and the tool suite of [LangChain](https://python.langchain.com/en/latest/index.html) in order to build an application which would be either impossible or just with a tedious amount of work to achieve otherwise.\n",
    "\n",
    "For this demo we are going to build a system which can do one of the subtasks for this course, creating a podcast script, fully automatically.\n",
    "We will use our lecture slides to extract information about the requirements and the topic, search wikipedia and the web for current information and use this to create our podcast script.\n",
    "\n",
    "This is done to show 3 things:\n",
    "\n",
    "- How powerful the tool sets have become in order to automate a wide array of problems through to be impossible to solve\n",
    "- The problem the educational sector faces with examination of the students\n",
    "- The advantage people who have access to such tools (GPT4, ChatGPT Plugins) will have over those who don't\n",
    "\n",
    "The second and third point are especially problematic as OpenAI is working on a [Plugins System](https://openai.com/blog/chatgpt-plugins) which will be available to the average user and will be able to more or less do the same thing shown here.\n",
    "\n",
    "> All personal inforation in the lecture script was removed for privacy reasons.\n",
    "\n",
    "> This demo is inspired by the work done on [Auto GPT](https://github.com/Significant-Gravitas/Auto-GPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "import wikipedia\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup chatgpt\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.9)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the topic for which we want to create the podcast for\n",
    "T1 = 'Large Language Models'\n",
    "T2 = 'Time series analysis'\n",
    "T3 = 'Face aging'\n",
    "T4 = 'Colorize'\n",
    "T5 = 'Recommendation Systems'\n",
    "T6 = 'Bayesian modelling'\n",
    "T7 = 'Process Mining'\n",
    "T8 = 'Voice Recognition'\n",
    "T9 = 'Dialect in speech recognition'\n",
    "T10 = 'Transfer Learning in Speech Recognition'\n",
    "T11 = 'Auto Deep Learning'\n",
    "T12 = 'Automatic feature extraction'\n",
    "\n",
    "topic = T1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering information from the lecture notes\n",
    "\n",
    "We can use ChatGPT to interpret the lecture slides which lists out the requirements for the podcast.\n",
    "By loading this document we can extract the information contained in there. Like:\n",
    "\n",
    "- the duration which the podcast should have (20 min)\n",
    "- How many words the use in the script for the podcast\n",
    "- Which topics should be covered\n",
    "\n",
    "> In the end we are not using the number of words calculated because it worsens the quality of the output, but there might be a good way to incorporate this by experimenting with other prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document loaders\n",
    "loader = TextLoader(file_path='../documents/VL1-1-processed-eng.txt', encoding='utf-8')\n",
    "document_content = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n",
    "split_content = text_splitter.split_documents(document_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)  # type: ignore\n",
    "embeddings_search = Chroma.from_documents(split_content, embeddings)\n",
    "embeddings_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt template to get usable results\n",
    "prompt_template_text_document = \"\"\"\n",
    "Instruction:\n",
    "- Use the following pieces of context to answer the question at the end.\n",
    "- If you don't know the answer output: NULL\n",
    "- Just answer the question without providing any additional information\n",
    "\n",
    "Context:\n",
    "    {context}\n",
    "\n",
    "Question:\n",
    "    {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_documents = PromptTemplate(template=prompt_template_text_document, input_variables=['context', 'question'])\n",
    "chain_type_kwargs = {'prompt': prompt_template_documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever\n",
    "# usage with prompt templates see: https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
    "# to see the source documents set: return_source_documents=True\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=embeddings_search.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the duration of the podcast in minutes\n",
    "query_duration = 'Which time duration should the podcast have?'\n",
    "res_duration = qa.run(query_duration)\n",
    "res_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't used later but still an interesting application to use\n",
    "prompt_template_text_num_words = \"\"\"\n",
    "Instructions:\n",
    "  - if you are not sure make an estimate\n",
    "  - output just a number\n",
    "  - don't use any text in the answer like for example \"aproximately\" or \"words\"\n",
    "\n",
    "Context:\n",
    "  {context}\n",
    "\n",
    "Question:\n",
    "  How many words should I use in my podcast script to be able to talk the entire duration if a human speaks at 150 words per minute?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template_num_words = PromptTemplate(template=prompt_template_text_num_words, input_variables=['context'])\n",
    "num_words_chain = LLMChain(llm=llm, prompt=prompt_template_num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ask chatgpt how many words to use for the podcast\n",
    "res_words = num_words_chain.run(context=res_duration)\n",
    "res_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Use the output prompt to match the number of words and convert it to a int value\n",
    "\n",
    "    :param text: the prompt the tells us the number of words to use\n",
    "    :returns: the number specified in the text or 1000 if no number could be found\n",
    "    \"\"\"\n",
    "    max_tokens = 2048\n",
    "    regex = r\"\\b(\\d+[,.]\\d+|\\d+)\\b\"\n",
    "    results: list[str] = re.findall(regex, text, re.MULTILINE)\n",
    "    if not results:\n",
    "        return max_tokens\n",
    "\n",
    "    no_comma = re.sub(r'[,]', '', results[-1])\n",
    "    float_result = float(no_comma)\n",
    "    suggestion = round(float_result)\n",
    "    return max_tokens if suggestion > max_tokens else suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output from chatgpt to an integer\n",
    "num_words_to_use = get_num_words(res_words)\n",
    "num_words_to_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for the topics which should be used in the podcast\n",
    "query_topics = f'Which topics should be covered in the podcast about {topic}?'\n",
    "res_topics = qa.run(query_topics)\n",
    "res_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the topics in a way to be interpretable\n",
    "formatted_topics = re.split(r'[,|\\n|?]', res_topics)\n",
    "formatted_topics = [topic.strip() for topic in formatted_topics if not re.search(r'^$', topic)]\n",
    "formatted_topics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Doing research about the topics\n",
    "\n",
    "As ChatGPT has a training cutoff in September 2021 there might be more current information important to the topic out there.\n",
    "Of course we want to include this to achieve the best possible result and up to date responses.\n",
    "\n",
    "Now that we found out which requirements and topics we should use we can now do research regarding these.\n",
    "For this we can use a variety of tools (also langchain integrations). First we are using Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_wikipedia(topic: str):\n",
    "    \"\"\"\n",
    "    Ask wikipedia for the summary of the topic.\n",
    "    Note: results my be inaccurate!\n",
    "\n",
    "    :param topic: the topic to ask wikipedia for\n",
    "    :returns: the summary of the topic or an empty string if nothing was found\n",
    "    \"\"\"\n",
    "    time.sleep(0.3)\n",
    "    search = wikipedia.search(topic)\n",
    "    if not search:\n",
    "        return ''\n",
    "\n",
    "    try:\n",
    "        return wikipedia.summary(search[0], sentences=5)\n",
    "    except wikipedia.PageError:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all wiki research for the topics and all subtopics\n",
    "wiki_research = '\\n'.join([ask_wikipedia(topic) for topic in formatted_topics])\n",
    "wiki_research\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate podcast script\n",
    "\n",
    "Now that we have done our research about the topic we can generate our podcast script. As the output length of the model varies and the number of output tokens is set to a default max of 2046 and an absolute max of 4092, we possibly need to do multiple runs to get a full script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_text_script = \"\"\"\n",
    "Sub Topics:\n",
    "  {sub_topics}\n",
    "\n",
    "Context:\n",
    "  \\\"{context}\\\"\n",
    "\n",
    "Podcast Participants:\n",
    "  - Host\n",
    "  - Expert\n",
    "\n",
    "Previous Section of the Podcast Script:\n",
    "  \\\"{previous_section}\\\"\n",
    "\n",
    "Task:\n",
    "  - Your task is to write a podcast script about \\\"{topic}\\\".\n",
    "  - The Sub Topics refine the main topic and need to be addressed!\n",
    "  - Use your own knowledge and the one provided in Context if you think it fit the topic.\n",
    "  - Continue from the previous section and output the new content.\n",
    "  - If you think you are done output [END]\n",
    "\n",
    "\"\"\"\n",
    "prompt_template_podcast = PromptTemplate(template=prompt_template_text_script, input_variables=['sub_topics', 'context', 'topic', 'previous_section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_repeated_chain(chain: LLMChain, max_iterations: int = 8, stop_word: str = '[END]', **chain_kwargs) -> str:\n",
    "    \"\"\"\n",
    "    run the podcast chain until the podcast is the stop word is found or the max_iterations is reached\n",
    "\n",
    "    :param chain: the chain to run\n",
    "    :param max_iterations: the maximum number of iterations to run the chain\n",
    "    :param stop_word: the word to stop the chain\n",
    "    :param chain_kwargs: the kwargs to pass to the chain\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    found_end = False\n",
    "    script = ''\n",
    "    while not found_end:\n",
    "        if i >= max_iterations:\n",
    "            print(f'[WARNING] could not finish task in {max_iterations} iterations')\n",
    "            break\n",
    "        output = chain.run(**chain_kwargs)\n",
    "        script += output\n",
    "        if 'previous_section' in chain_kwargs:\n",
    "            chain_kwargs['previous_section'] = output\n",
    "        found_end = stop_word in output\n",
    "        i += 1\n",
    "        print(f'iteration {i} of {max_iterations} finished')\n",
    "\n",
    "    return script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_topics = '\\n'.join(f'  - {sub_topic}' for sub_topic in formatted_topics)\n",
    "sub_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_chain = LLMChain(llm=llm, prompt=prompt_template_podcast)\n",
    "\n",
    "podcast_script = run_repeated_chain(podcast_chain, sub_topics=sub_topics, context=wiki_research, topic=topic, previous_section='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it to a file for later use\n",
    "with open('../out/podcast_script_wiki.txt', 'a') as script_file:\n",
    "    script_file.write(podcast_script)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tools and agents\n",
    "\n",
    "What we could do using other frameworks and manual methods like before (wikipedia), we can now use integrated tools in combination with agents to automate our process further.\n",
    "\n",
    "So here we will do an example how to leverage current web search results in order to get the most up to date information we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(['serpapi'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a web research for all topics\n",
    "all_topics = [topic, *formatted_topics]\n",
    "web_research = []\n",
    "\n",
    "for topic in all_topics:\n",
    "    agent_res = agent.run(f\"Task: Do a thorough web research about {topic}. Provide at least 3 sentences of information.\")\n",
    "    web_research.append(agent_res)\n",
    "\n",
    "web_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_web_research = '\\n\\n'.join(web_research)\n",
    "formatted_web_research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the podcast script like before just now with the serpapi agent web research\n",
    "podcast_chain = LLMChain(llm=llm, prompt=prompt_template_podcast)\n",
    "\n",
    "podcast_script_web = run_repeated_chain(podcast_chain, sub_topics=sub_topics, context=formatted_web_research, topic=topic, previous_section='')\n",
    "podcast_script_web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../out/podcast_script_web.txt', 'w') as script_file:\n",
    "    script_file.write(podcast_script_web)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ending Thoughts\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "As I have done my research, tested out various models - closed and open source - and also programmed the demo I have noticed the importance of choosing approximately the right prompt.\n",
    "Especially when I tried out the models from [GPT4All](https://github.com/nomic-ai/gpt4all) and used them without a prompt template one thing became very clear: The model is not able to do anything without a prompt.\n",
    "Well it outputs text at least, but no which would make sense. To solve such problems we need use trial and error, as well as look for patterns in the dataset.\n",
    "The model lives and falls with the right prompts and prompt templates. The field of [Prompt Engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is not longer a laughable matter but reality.\n",
    "\n",
    "### API Costs\n",
    "\n",
    "As we stand now most of the usable LLMs are hidden behind a paywall. This would be bearable if it was only one service we would have to pay for, but as your requirements increase it would not be surprising to be subscribed to a number of APIs. Especially running applications in an \"AutoGPT manner\" is very quickly going to drain your wallet. Also make sure you have a credit card, otherwise you won't be able to subscribe to almost any API.\n",
    "\n",
    "### Privacy\n",
    "\n",
    "There are serious privacy concerns when using ChatGPT to work with your private data.\n",
    "Not only can you be sure that they will know your most private information (which is sadly reality nowadays). Especially with those LLMs we are now more able than ever you analyze a large amount of unstructured data which prevents you to stay hidden in the masses. Another concern is that this data will probably end up in the training data and god help you if another user gets output from your private data.\n",
    "\n",
    "### Open source models\n",
    "\n",
    "[ChatBot Arena](https://chat.lmsys.org/?arena)\n",
    "\n",
    "As _GPT3.5-turbo_ and _GPT4_ dominate the current market of large language models the open source variants start rising more and more to the occasion.\n",
    "![ChatGPT, Bard, Vicuna13B performance](https://lmsys.org/images/blog/vicuna/chart.svg)\n",
    "We can see that especially the _Vicuna-13B_ models reaches and astounding [performance of 92%](https://lmsys.org/blog/2023-03-30-vicuna/) compared to the _GPT3.5-turbo_ model used for.\n",
    "This is especially astounding is we look at the number of parameters for each model:\n",
    "\n",
    "- _GPT3.5-turbo_: 175B\n",
    "- _Vicuna-13B_: 13B\n",
    "\n",
    "As close as this seems there are several problems the open source models nowadays face:\n",
    "\n",
    "1. Training resources\n",
    "   The first one is very obvious and that's the training cost involved. As an average user it is nearly impossible to train even a 13B parameter model. Maybe you are able to do some fine tuning but that's all at best. So the community either relies on a couple individuals or companies who don't have as much training as the monetized competitors.\n",
    "   Addressing these problems there is a very interesting open source project out there which tries it's hand on decentralized training named [petals](https://github.com/bigscience-workshop/petals). But it is still young and we are yet to see what to come.\n",
    "\n",
    "2. Training datasets\n",
    "   This is another very simple one. Even though there is open source datasets for training large language models they don't even come close to the ones kept at companies like Google, Microsoft, Facebook and OpenAI.\n",
    "   To solve this problem the open source community found a very easy, but interesting approach. The use ChatGPT output to train their models. On the other hand this means of course that the models exclusively trained with that method cannot become better than it trainer.\n",
    "\n",
    "3. Imitating output vs understanding output\n",
    "   The big problem noticed with open source models is that there seems to be a divergence between imitating vs understanding the output. As we established before a lot of open source models are trained using ChatGPT inputs and outputs.\n",
    "   Leaving the other problems aside they just seemed to imitate ChatGPT which is a big problem for transfer, zero or one shot learning.\n",
    "   To address this other approaches were chosen where both _GPT3.5-turbo_ and _GPT4_ were used and instructed to also explain it's reasoning process which dramatically improved performance.\n",
    "\n",
    "But there is very good news for us if we can trust the contents of a [leaked memo by google](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). This memo states there is a high probability that the open source models will close the gap in the near future - to themselves and even to OpenAI.\n",
    "\n",
    "### Interesting Tools\n",
    "\n",
    "- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)\n",
    "  - AI agent doing everything possible to achieve whichever goal you give it\n",
    "- [perplexity.ai](https://www.perplexity.ai/)\n",
    "  - free AI search engine which uses and shows it's sources\n",
    "- [privateGPT](https://github.com/imartinez/privateGPT)\n",
    "  - Document search engine which uses just private and open source models and technology\n",
    "- [PentestGPT](https://github.com/GreyDGL/PentestGPT)\n",
    "  - Automatic penetration testing / hacking tool. Similar to AutoGPT\n",
    "- [MiniGPT](https://github.com/Vision-CAIR/MiniGPT-4)\n",
    "  - open source chatbot with vision / image understanding\n",
    "- [petals](https://github.com/bigscience-workshop/petals)\n",
    "  - open source distributed training and usage of a BLOOM 176B parameter model\n",
    "- [GPT4All](https://github.com/nomic-ai/gpt4all)\n",
    "  - collection of open source LLMs\n",
    "\n",
    "### Implications for the average user\n",
    "\n",
    "As we have seen with this example and also even more so when looking at the [interesting tools](#interesting-tools), applications leveraging the power of LLMs are rapidly getting more powerful and sophisticated.\n",
    "Once public exposure, cost, variety and usability of LLMs increase we will see a new era of applications which will change our lives forever.\n",
    "We should start to review the systems which are in place and adapt them.\n",
    "For example taking the educational sector. We are still using the same methods of learning which were used 100 years ago. Even though we have the technology to make learning more interactive and fun we still rely on the same old methods.\n",
    "At the same time there is also a shadow side to this. We problems with privacy, people fully relying on ChatGPT and not thinking anymore for themselves and many more.\n",
    "\n",
    "Rather than complain we should do following things:\n",
    "\n",
    "- Talk about the capabilities of such tools\n",
    "- Talk about the risks and disadvantages\n",
    "- Talk about how to incorporate such tools in our lives\n",
    "- Talk about boundaries and limits\n",
    "\n",
    "Before this is not done we cannot create a promising strategy of how to solve the problems we are facing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-ki-seminar-p9aLL8gI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
